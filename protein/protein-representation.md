ðŸ‘ˆ [Back to Home Page](../README.md/)

# Protein Representation

- [Protein Representation](#protein-representation)
  - [Protein Language model](#protein-language-model)
  - [Structure representation](#structure-representation)


ðŸ“˜ Related paper reading list: [awesome-protein-representation-learning](https://github.com/LirongWu/awesome-protein-representation-learning)



## Protein Language model

|   |Title|Pub.&Year|Notes|
|---|-----|---------|-----|
|TAPE|[Evaluating Protein Transfer Learning with TAPE](http://biorxiv.org/lookup/doi/10.1101/676825)|NIPS '2019|
|ESM-1b|[Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences](https://www.pnas.org/doi/full/10.1073/pnas.2016239118)|PNAS '2020|
|ProtTrans|[ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning](https://ieeexplore.ieee.org/document/9477085)|TPAMI '2021|
||[MSA Transformer](https://www.biorxiv.org/content/10.1101/2021.02.12.430858v1)||
|[Survey]|[Learning functional properties of proteins with language models](https://www.nature.com/articles/s42256-022-00457-9)|NMI '2022|
|[Benchmark]|[PEER: A Comprehensive and Multi-Task Benchmark for Protein Sequence Understanding](https://arxiv.org/abs/2206.02096)|NIPS '2022|
||[Large language models generate functional protein sequences across diverse families](https://www.nature.com/articles/s41587-022-01618-2)|NBT '2022||
|ProtGPT2|[ProtGPT2 is a deep unsupervised language model for protein design](https://www.nature.com/articles/s41467-022-32007-7)|NC '2022|
|[Survey]|[A Survey on Protein Representation Learning: Retrospect and Prospect](http://arxiv.org/abs/2301.00813)|Arxiv '2023|[paper list](https://github.com/LirongWu/awesome-protein-representation-learning)|
|xTrimoPGLM|[xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein](https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1)|2023|
|SaProt|[SaProt: Protein Language Modeling with Structure-aware Vocabulary](https://www.biorxiv.org/content/10.1101/2023.10.01.560349v2)|ICLR '2024|[code](https://github.com/westlake-repl/SaProt)|
|[Benchmark]|[Feature Reuse and Scaling: Understanding Transfer Learning with Protein Language Models](https://proceedings.mlr.press/v235/li24a.html)|ICML '2024|
||[ESM All-Atom: Multi-scale Protein Language Model for Unified Molecular Modeling](http://arxiv.org/abs/2403.12995)|ICML '2024|
|AMPLIFY|[Protein Language Models: Is Scaling Necessary?](https://www.biorxiv.org/content/10.1101/2024.09.23.614603v1#:~:text=Protein%20language%20models%20(pLMs)%20pre-trained%20on%20these)|BioRxiv '2024|
|Ginkgo-AA|[A Protein Sequence LLM Trained on 2 Billion Proprietary Sequences](https://www.ginkgobioworks.com/2024/09/17/aa-0-protein-llm-technical-review/#:~:text=We%20compare%20the%20performance%20of%20AA-0%20to%20ESM-2%20on%20popular)|blog|650M|

ðŸ‘† [Back to Top](#protein-representation)


## Structure representation

|   |Title|Pub.&Year|Notes|
|---|-----|---------|-----|
||||
||||


ðŸ‘† [Back to Top](#protein-representation)
